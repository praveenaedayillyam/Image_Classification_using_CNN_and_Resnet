# ğŸ–¼ï¸ Image Classification Using CNN and Transfer Learning (ResNet50)
- ğŸ“Œ Project Overview<br>

This project focuses on multi-class image classification using two deep learning approaches: <br>

Custom Convolutional Neural Network (CNN) <br>

Transfer Learning using ResNet50<br>

The goal is to classify natural scene images into six categories using TensorFlow and Keras, and to compare the performance of a custom-built CNN with a pre-trained deep learning model.<br>

- ğŸ§  What is Transfer Learning?<br>

Transfer Learning is a machine learning technique where a pre-trained model, trained on a large dataset (like ImageNet), is reused for a new but related task.<br>

Instead of training a deep network from scratch, the model leverages previously learned features such as:<br>

edges<br>

textures<br>

shapes<br>

This significantly reduces training time and often improves performance, especially when the dataset is limited.<br>

- ğŸ—‚ï¸ Dataset<br>

The dataset consists of natural scene images categorized into six classes:<br>

ğŸ¢ buildings<br>

ğŸŒ² forest<br>

â„ï¸ glacier<br>

â›°ï¸ mountain<br>

ğŸŒŠ sea<br>

ğŸ›£ï¸ street<br>

Dataset Details <br>

Training images: 14,034<br>

Testing images: 3,000<br>

Image size: 64 Ã— 64 Ã— 3<br>

Format: Directory-based (compatible with ImageDataGenerator)
<br>
- âš™ï¸ Technologies & Libraries Used<br>

Python<br>

TensorFlow / Keras<br>

NumPy<br>

OpenCV<br>

Matplotlib<br>

Seaborn<br>

Scikit-learn<br>

gdown (Google Drive download)<br>

- ğŸ”„ Data Preprocessing & Augmentation<br>

Rescaled pixel values to [0,1]<br>

Applied data augmentation:<br>

Zoom<br>

Width & height shift<br>

Used ImageDataGenerator for:<br>

Training<br>

Validation<br>

Testing<br>

- ğŸ§© Models Implemented<br>
ğŸ”¹ 1. Custom CNN Model<br>

Architecture highlights:<br>

Convolution + ReLU<br>

MaxPooling<br>

Batch Normalization<br>

Dropout (to reduce overfitting)<br>

Fully connected dense layers<br>

Softmax output layer (6 classes)<br>

Total Parameters: ~206K<br>
Optimizer: Adam<br>
Loss Function: Categorical Crossentropy<br>

- ğŸ”¹ 2. Transfer Learning â€“ ResNet50<br>

Key points:<br>

Pre-trained on ImageNet<br>

include_top=False<br>

Global Average Pooling<br>

Fine-tuning enabled<br>

Additional dense layers for classification<br>

Total Parameters: ~23M<br>
Optimizer: Adam<br>
Loss Function: Categorical Crossentropy<br>

- ğŸ“Š Model Performance Comparison<br>
âœ… CNN Model Performance<br>

Test Accuracy: 75% <br>

Better generalization on this dataset <br>

Stable training & validation curves <br>

Classification Report (CNN): <br>

Accuracy: 0.75 <br>
Macro Avg F1-score: 0.75 <br>
Weighted Avg F1-score: 0.74 <br>

- âš ï¸ ResNet50 Model Performance <br>

Test Accuracy: 63% <br>

Overfitting observed <br>

Requires better fine-tuning and learning rate scheduling<br>

Classification Report (ResNet50):<br>

Accuracy: 0.63<br>
Macro Avg F1-score: 0.63<br>
Weighted Avg F1-score: 0.62<br>

- ğŸ“ˆ Visualizations<br>

Sample training images with labels<br>

Training vs Validation Accuracy plots<br>

Prediction vs Actual comparison on test images<br>

Class-wise precision, recall, and F1-score<br>

ğŸ§ª Model Testing<br>

Predictions generated on unseen test dataset<br>

Compared predicted labels with actual labels<br>

Evaluated using:<br>

Accuracy<br>

Precision<br>

Recall<br>

F1-score<br>

Confusion Matrix<br>

- ğŸ Conclusion<br>

The Custom CNN outperformed ResNet50 on this dataset<br>

Transfer learning requires careful fine-tuning<br>

Smaller image size (64Ã—64) may limit deep models like ResNet50<br>

CNN proved to be more efficient and stable for this task<br>

- ğŸš€ Future Improvements<br>

Freeze initial ResNet50 layers and fine-tune selectively<br>

Increase input image resolution (e.g., 128Ã—128)<br>

Apply learning rate scheduling<br>

Try other transfer learning models:<br>

MobileNet <br>

EfficientNet

Deploy model using Streamlit or Flask
